\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}

% Intro til A3C

\section{Asynchronous Advantage Actor-Critic}\label{sec:a3c}

A problem with the Actor-Critic method is that training the model can become very time consuming,
when it is solving challenging tasks such as Atari games.
To speed up the training process it is possible to use 
multiple Actor-Critic models to 
solve the problem locally in parallel.
These models share a set of parameters, $\theta$ and $\mathbf{w}$, which they update asynchronously,
and thus, each model can benefit from the experience gained by the other models.

The approach presented in \cite{a3c}, called the \textit{Asynchronous Advantage Actor Critic} (A3C) method,
use several local models and a single global model, whose only purpose is to keep track of the global parameters,
to train asynchronously.
Each of the local models use a copy of the global parameters, $\theta'$ and $\mathbf{w}'$, to run through a problem
and update the global parameters based on the local experience.
The global update should be performed such that
the elegibility of each component of the gradient determines the size of the
gradient step.

A key difference between each model in the A3C method and the model of the single-threaded Actor-Critic method,
is that the models are no longer evaluated by the TD-error,
but instead by the \textit{advantage} gained from performing an action.
The advantage, $A$, is defined as the difference between the estimated value of
a state and the actual return from that state.
\begin{equation}
    A = R - \hat{v}(s, \mathbf{w}')
\end{equation}
To decrease the amount of insignificant updates to the global
model, the local models are allowed to perform several actions
before performing an asynchronous update, that are stored in a \textit{short-term
replay memory}.
The replay memory contains the experienced rewards, states and actions,
which are used to compute the gradient steps.

To compute the advantage we need to estimate the return $R$.
The return is based partly on the value estimate, such that we will only
need to keep the most recent experience gained in the replay memory.
To estimate $R$, we can use the experience in the following way
\begin{equation}
    R = r_i + \gamma R
\end{equation}
for all $r_i$ in the replay memory, starting from the last reward received.
We initalize $R$ to be zero if the last state in the replay memory was a terminal state,
and the estimated value of the state, $\hat{v}(s, \mathbf{w}')$, otherwise.

The aim of sampling multiple actions is to accumulate the directional steps
to increase the chance of encountering a significant state change.
The accumulation of the policy gradients resembles the gradient ascent from equation
\ref{eq:ac_theta}, with the exception that the advantage is used to evaluate
the performance of the policy, instead of the TD-error.
\begin{equation}
    d\theta' = d\theta' + \nabla_{\theta'} \log \pi(a|s, \theta')(R - \hat{v}(s, \mathbf{w}'))
\end{equation}
As in equation \ref{eq:ac_theta}, taking the gradient of $\log(\pi(a|s, \theta'))$
is an attempt to normalize the influence of actions, no matter their probability
of being sampled.
The advantage is an indicator of the performance of the policy, and decides
which way the gradient step should be taken.

In the previous sections we updated the value function proportionally to the
one-step TD-error, but since we estimate the actual return, $R$,
we can use the advantage, $R - \hat{v}(s, \mathbf{w'})$,
as an error measure instead.
We want the expected return of a state to be as close to the experienced return as possible.
In order to emphasize this, we square the advantage, to express the magnitude of the
gradient step, that should be taken in the asynchronous update.
In \cite{a3c} this update is specified as
\begin{equation}
    d\mathbf{w}' = d\mathbf{w}' + \frac{\partial (R - \hat{v}(s, \mathbf{w'}))^2}{\partial \mathbf{w}'}
\end{equation}
which is the same as taking the gradient with respect to $\mathbf{w}'$ since
\begin{equation*}
    \begin{aligned}
        \frac{\partial (R - \hat{v}(s, \mathbf{w}'))^2}{\partial \mathbf{w}'} & = \bigg(
            \frac{\partial (R - \hat{v}(s, \mathbf{w}'))^2}{\partial \mathbf{w}'_1},
            \frac{\partial (R - \hat{v}(s, \mathbf{w}'))^2}{\partial \mathbf{w}'_2}, \dots,
            \frac{\partial (R - \hat{v}(s, \mathbf{w}'))^2}{\partial \mathbf{w}'_d} \bigg) \\
            & = \nabla_{\mathbf{w}'} (R - \hat{v}(s, \mathbf{w}'))^2
    \end{aligned}
\end{equation*}
where $\mathbf{w}'_1, \mathbf{w}'_2, \dots, \mathbf{w}'_d$ are the components of $\mathbf{w}'$.
Whenever the advantage is large, we want the gradient step to be big as well,
and likewise, when the advantage is small we want to take small steps as well.

As an extra measure to favor exploration, an \textit{entropy} has been added
to the policy approximator of the local models.
The entropy of the policy corresponds to the spread of action probabilities.
This means that the entropy will be large when the policy is close to being
uniform and small when the probabilities approach determinism.
In short, the entropy is used to discourage premature convergence
by encouraging the model to be more conservative about how sure it is
of which action is correct.
Using the entropy, $H(\pi(s, \theta'))$, alters the accumulation of the gradients
of the policy, such that
\begin{equation}
    \begin{aligned}
        d\theta' & = d\theta' + \nabla_{\theta'} \log \pi(a|s, \theta')(R - \hat{v}(s, \mathbf{w}')) + \beta \nabla_{\theta'} H(\pi(s, \theta'))\\
                 & = d\theta' + \nabla_{\theta'} (\log \pi(a|s, \theta')(R - \hat{v}(s, \mathbf{w}')) + \beta H(\pi(s, \theta')))
    \end{aligned}
\end{equation}
where $0 \leq \beta$ expresses how much influence the entropy should have.

Now that we can accumulate the gradients of the policy and value estimator, we need to update the
global parameters
To perform the asynchronous update, we use an approach called \textit{Root Mean Square propagation} (RMSProp).
RMSProp is an approach to \textit{gradient descent} 
that uses the magnitude of the recent gradients to
determine the step-size of each update.
The update is given by 
\begin{equation}
    \theta = \theta - \eta \frac{\Delta \theta}{\sqrt{g + \epsilon}}
\end{equation}
where $0 < \eta$ is a constant used to limit the step-size of the gradient descent, $\Delta \theta$ is
the update to made, and 
$g$ is the magnitude of the gradient.
In this way $g$ has the same function as the elegibility traces discussed in
section \ref{sec:actor_critic_el}.
\begin{equation}
    g = \alpha g + (1 - \alpha) \Delta \theta^2
\end{equation}
To create a balance between old and new trends in the gradient
, a constant, $0 < \alpha < 1$ is used as a decay rate,
determining how much each new update can influence the magnitude and how much the previous magnitude fades away.
In this project $g$ is shared between all local models, which should have a stabilizing
effect, since “wrong” gradient steps won't influence the parameters as much.

The primary advantage of the A3C algorithm compared to the regular Actor-Critic method
is that the amount of time consumed training the model is far lower.
By running multiple Actor-Critic models in parallel, more paths through the environment are explored
at the same time.
All local models try to maximize the return from the state that they're in, which means that
the global parameters are influenced by a lot of different factors at the same time.

\end{document}
