\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}


\subsection{Updating the weights of a network}

In section \ref{sec:actor_critic} and \ref{sec:a3c} we discussed different
approaches available to update the weights of the policy and value estimator.
These approaches are based on the directional values
expressed by the gradient of the estimator, or an error measure containing
the estimator.
Taking the gradient of a function $f : \R^n \to \R$ with respects to
some parameters $x \in \R^n$ is given by
\begin{equation}
    \nabla_x f(x)
    = \begin{pmatrix}
        \frac{\partial f(x)}{\partial x_1}\\
        \frac{\partial f(x)}{\partial x_2}\\
        \vdots\\
        \frac{\partial f(x)}{\partial x_n}
      \end{pmatrix}
\end{equation}

Now, in a neural netwokr it is difficult to take the gradient of the
entire network at once, since it consists of a number of layers
using different activation functions and a lot of parameters.
To solve this issue the general approach to computing the gradient of a network
is to use \textit{back-propagation} as presented in \cite{IgelBackProp}
and \cite{DeepLearningBook}.

Consider a network consisting of $d$ input neurons, $M$ hidden neruons and $K$ output neurons,
estimating the function $f : \R^d \to \R^K$.
Each neuron can be denoted as $z_i$, where neurons for which $i < d$ are the input neurons, $d < i \leq M + d$ are the hiden neurons
and $M + d < i \leq M + d + K$ are the output neurons.
As described in section \ref{sec:lnc} each neuron $z_i$ can be seen as a weighted activation of its input.
For the neurons in the hidden layers and the output layer, this means $z_i = h(a_i)$, where 
$a_i = \sum_{j} w_{ij} * z_{j}$ is the weighted sum of the input to the ith neuron and $h$ is an arbitrary
activation function.

Previously we updated the weights of an estimator with regards to some performance measure $\rho$ 
describing how well the policy and value functions are performing. 
Now, we want to find the directional changes for all weights in the network
in regards to the performance measure.
If all $h$ in the network are differentiable this results in the partial derivatives
\begin{equation}\label{part}
    \frac{\partial \rho}{\partial w_{ij}}
\end{equation}
describing the directional change for each weighted connection in the network from $z_j$ to $z_i$.

If we start by looking at this partial derivative in the K output neurons of the
neural network, we can apply the chain rule of calculus to \ref{part},
\begin{equation}
    \frac{\partial \rho}{\partial w_{ij}} = \frac{\partial \rho}{\partial a_i} \frac{\partial a_i}{\partial w_{ij}} 
\end{equation}
since $w_{ij}$ is a part of $a_i$.
$\frac{\partial \rho}{\partial a_i}$ is denoted as $\delta_i$ and $\frac{\partial a_i}{\partial w_{ij}} = z_i$ since
$\frac{\partial a_i}{\partial w_{ij}} = \frac{\partial}{\partial w_{ij}} \sum_{d} w_{id} * z_d = z_j$.
This means
\begin{equation}\label{eq:der}
    \frac{\partial \rho}{\partial w_{ij}} = \delta_i * z_j  
\end{equation}
For each of the output units, $\delta$ can be found as
\begin{equation}
    \begin{aligned}
        \delta_i & = \frac{\partial \rho}{\partial a_i}\\
        & = \frac{\partial \rho}{\partial z_i} \frac{\partial z_i}{\partial a_i}\\
        & = h'(a_i) \frac{\partial \rho}{\partial z_i} 
    \end{aligned}
\end{equation}
since $\frac{\partial z_i}{\partial a_i} =  \frac{\partial}{\partial a_i} h(a_i) = h'(a_i)$.
Therefore the partial derivative of the performance measure with respects to a weight from
an output neuron can be found as
\begin{equation}
    \frac{\partial \rho}{\partial w_{ij}} = h'(a_i) \frac{\partial \rho}{\partial z_i} * z_j
\end{equation}
Now, this only holds for the output neurons because their output isn't
used by any other neuron.
For the hidden neurons we have to take all subsequent neurons into account as
well, since their output is propagated forward through the network.
Thus,
\begin{equation}
    \delta_i = \sum\limits_{k=i+1}^{M + d + K} \frac{\partial \rho}{\partial a_k} \frac{\partial a_k}{\partial a_i} 
\end{equation}
for all $d > i \leq M + d + K$ - the hidden neurons.
Since $\frac{\partial \rho}{\partial a_k} = \delta_k$ and
$\frac{\partial a_k}{\partial a_i} = \frac{\partial a_k}{\partial z_i} \frac{\partial z_i}{\partial a_i}$ we
can derive $\delta_i$ as
\begin{equation}
    \begin{aligned}
        \delta_i & = \sum\limits_{k=i+1}^{M + d + K} \delta_k \frac{\partial a_k}{\partial z_i} \frac{\partial z_i}{\partial a_i}\\
        & = \sum\limits_{k=i+1}^{M + d + K} \delta_k * w_{ki} *  h'(a_i)\\
        & = h'(a_i) \sum\limits_{k=i+1}^{M + d + K} \delta_k * w_{ki}
    \end{aligned}
\end{equation}
because $\frac{\partial a_k}{\partial z_i} = \frac{\partial}{\partial z_i} \sum_{j} w_{kj} * z_j = w_{ki}$.
Using equation \ref{eq:der} we can then compute the partial derivatives for all output and hidden neurons.
It is important to note that the partial derivatives have to be computed in the reverse order,
since each the derivative in each neuron is dependant on the ones from all the neurons later
in the network.

\end{document}
