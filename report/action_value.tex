\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}

\maketitle

\section{Value Functions}

Most reinforcement learning algorithms estimates a value function, that compute how profitable it is to be in a specific state, or take a specific action given the state. The term \textit{"profitable"} means what the expected future reward will become. Generally in reinforcement learning we have two value function \textit{state value function} and \textit{action value function}.

\subsection{State Value Function}

\subsection{Action Value Function}

The action value function calculating the expected reward for taking action $a$ in state $s$ under policy $\pi$, is denoted by $q_{\pi}(s, a)$, and defines as
\\
\begin{equation} \label{eq:Action Value Function}
    q_{\pi}(s, a) = \mathbb{E}\Big[G_{t} \Big| S_{t} = s, A_{t} = a\Big] \\
    = \mathbb{E}\Bigg[\sum_{k = 0}^{\infty}\gamma^{k}R_{t + k + 1} \Bigg| S_{t} = s, A_{t} = a \Bigg]
\end{equation}
Where $\gamma$ is the discount rate $0 \leq \gamma \leq 1$, and $\gamma^{k}R_{t + k + 1}$ is the discounted reward at time $t$, after being in state $s$. The discount rate describes the impact of how future rewards influence the return value of the action value function, since $\gamma^{k} \rightarrow 0$, when $k \rightarrow \infty$, unless $\gamma = 1$
\\ \\
By using equation \ref{eq:Action Value Function} we can compute the expected sum of discounted rewards, by taking action $a$ in state $s$ with policy $\pi$. Generally in reinforcement learning we want to maximizing the return value, for the value action function, we want to take action $a$ in state $s$ which maximizing the future discounted reward. We can rewrite equation \ref{eq:Action Value Function}, to a optimal action value function
\begin{equation}
\begin{split}
    q_{*}(s,a) &= \mathbb{E}\Bigg[R_{t + 1} + \gamma  \max\limits_{a^{'}} q_{*}(S_{t + 1}, a^{'}) \Bigg| S_{t} = s, A_{t} = t \Bigg] \\
    &= \sum_{s{'}, r} p(s^{'}, r \Big| s, a) \Big[r + \gamma \max\limits_{a^{'}}q_{*}(s{'}, a^{'})\Big]
\end{split}
\end{equation}
So the optimal action value function return the sum of discounted rewards by taking action $a$ in state $s$. The optimal action value function is recursive, for every element in the sum of the discounted rewards, we compute which action there maximize the discounted reward, until we end up in a terminate state.
\end{document}

