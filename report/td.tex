\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}

\maketitle

\section{Reinforcement Learning}



\subsection{Temporal Difference (TD) Learning}

Temporal difference (TD) learning is a technique used in reinforcement learning for predicting the total reward expected in the future. By using TD learning it's possible to update the value functions after each time step. We wish to update the value functions after each time step, instead of each episode, for stabilizing the learning with several minor updates. Updating the state value function for a state $S_{t}$ at time $t$ after running through an entire episode can be formalized as,
\begin{equation}\label{eq:td}
    V(S_{t}) \leftarrow V(S_{t}) + \alpha \big[G_{t} - V(S_{t}) \big]
\end{equation}
where $0 < \alpha \leq 1$ corresponding to the update rate of the value state function. The problem with this approach is that we have to wait until an episode is done, before we can update the state value function. We want to update the state value function dynamically, because we assume that the next estimation of the value is closer to the true return. One way for updating the value function dynamically is to estimate the value of a state and compare it to the estimated value in the subsequent state, this method is called \textit{TD(0)}.
\begin{equation}\label{eq:td2}
    V(S_{t}) \leftarrow V(S_{t}) + \alpha \big[R_{t + 1} + \gamma V(S_{t + 1}) - V(S_{t}) \big]
\end{equation}
Here we compute the difference between the estimated value for being in state $S_{t}$ and $S_{t + 1}$ plus $R_{t + 1}$, where $R_{t + 1}$ is the reward gained by the transition from state $S_{t}$ to $S_{t + 1}$. The expression $R_{t + 1} + \gamma V(S_{t + 1}) - V(S_{t})$, is called the \textit{TD error} for TD(0).
\begin{equation}
    \delta_{t} = R_{t + 1} + \gamma V(S_{t + 1}) - V(S_{t})
\end{equation}
So the generally in TD error is that $(V(S_{t + 1}) + R_{t + 1})$ should be a better estimate of the actually return than $V(S_{t})$ for state $S_{t}$, by the assumption of that the estimate made later in the episode is better, because it will be closer to a terminal state.  

\subsection{$n$-step return}
We mentioned TD(0) in the last section, is called one-step, because it change the value functions based on a estimate made one step later. TD learning can also be used for \textit{n-step backups}, where we compute the TD error between state $S_{t}$ and $S_{t + n}$. The $n$-step backup can formally be expressed as the $n$-step return,
\begin{equation}
    G_{t}^{(n)} = R_{t + 1} + \gamma R_{t + 2} + \dots + \gamma^{n - 1} R_{t + n} + \gamma^{n} V_{t + n - 1}(S_{t + n})
\end{equation}
where $n \geq 1$ is the number of time steps we look intro the future.




\end{document}
