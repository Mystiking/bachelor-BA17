\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}

\maketitle

\subsection{Temporal Difference Learning}\label{sec:td}

We now have a way to describe the value of a state, but we rarely know the
value function $v_\pi$ before beginning to solve a problem.
Even with knowledge of the entire state space and all available actions,
it is difficult to compute the true value function.
Instead we can construct an estimator $V(S_t)$ that
approximates $v_\pi(S_t)$. However, this means we need a way
to measure how close the estimator is to the true value function.

When a learning agent solves a problem, it gains experience
about the states it encounters and the rewards earned through its transitions.
Using this experience it is possible to construct an estimator 
of the state-value function,
\begin{equation}
    V(S_t) = V(S_t) + \alpha [G_t - V(S_t)]
\end{equation}
where $0 \leq \alpha \leq 1$ is a step-size parameter and
$G_t = \sum\limits_{k=0} \gamma^k R_{t+k+1}$.

This estimator only allows us to update the estimate, when a
terminal state is reached, since this is the time at which $G_t$ is known.
Such an approach is called an \textit{episodic} update scheme, since the
state-value estimator is only updated when a task is completed -
or in other words when an \textit{episode} is over.
Updating this way is inefficient because the estimator needs to complete
a lot of episodes to converge towards the true state-value function.

A different approach to estimating $V(S_t)$ everytime a new state is encountered.
From equation \ref{eq:sv} we know that the value function is partly based on its future
estimate, which we can extend to form an update scheme,
\begin{equation}\label{eq:up}
    V(S_t) = V(S_t) + \alpha  [R_{t+1} + \gamma  V(S_{t+1}) - V(S_t)]
\end{equation}
where we only have to take a single step into the future before we can 
update the estimate.
Updating the value estimator based on future results is the basis
of \textit{temporal difference learning}(TD) and the quantity $R_{t+1} + \gamma  V(S_{t+1}) - V(S_t)$
is defined as the \textit{one-step TD-error}.
This error describes the difference between the current estimate
and the estimate of the next state using the reward experienced after
transitioning to the new state.

Generally we want to minimize the TD-error, as the function $V(S_t)$
would then be as close to $v_\pi(S_t)$ as possible.
In particular if this error was 0 for all states, then
$V(S_t) = v_\pi(S_t)$ because 
\begin{equation}
    \begin{aligned}
    V(S_t)  & = V(S_t) + \alpha  [R_{t+1} + \gamma  V(S_{t+1}) - V(S_t)]   \\
           & = V(S_t) + \alpha  [V(S_t) - V(S_t)]  \\
           & = v_\pi(S_t) 
    \end{aligned}
\end{equation}

Thus, the TD-error provides us with a way to measure the performance of
the value estimator based on experienced returns.

\subsubsection{Multi-step TD learning}\label{sec:multi}

A benefit of the updating scheme from equation \ref{eq:up}, is that we are able to base
part of our estimate on an already existing estimate.
It can be useful to update the estimate after very state transitions, but sometimes
the changes over a single time step can be small and maybe even insignificant.
The one-step update uses the experience gained from a single transition to
estimate $G_t$.
We can extend the estimate of $G_t$ to take $n$ experienced rewards
into account, which is called as the \textit{n-step} return,
\begin{equation}
    G^{(n)}_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^n V(S_{t+n})
\end{equation}
where $1 \leq n$, $0 < t \leq T - n$ and $T - n$ is the nth time step before the terminal state
is reached.

Using the n-step return allows the learning agent to base its update on the next $n$ experienced
rewards, which means it is more likely to experience a significant change than the one-step return.
\begin{equation}
    \begin{aligned}    
        V(S_t) & = V(S_t) + \alpha [G^{(n)}_t - V(S_t)]\\
        & = V(S_t) + \alpha [R_{t+1} + \gamma R_{t+2} + \dots + \gamma^n V(S_{t+n}) - V(S_t)]
    \end{aligned}
\end{equation}
This way of updating the estimate is especially beneficial in tasks that only sometimes returns a reward. 
For example in most Atari games a score is only given to the player when an objective has been completed.
We want to find these significant changes because the entire chain of estimates 
will be affected, making the estimate converge faster towards the real value function.

\subsubsection{Eligibility traces}\label{sec:et}

% Intro til parameterized functions
So far we have described the state-value function as a mapping from states to values.
A problem with this approach is that it becomes infeasable to maintain 
when the state space is continuous, since there is an infinite number of states.
To estimate the state-value function in a continous state space we can use a
parameterized function $\hat{v}(s, \mathbf{w})$ with weights $\mathbf{w} \in \R^d$.
Thus, instead of keeping track of the last $n$ steps, the estimator $\hat{v}(s, \mathbf{w})$
only needs to keep track of the weight vector $\mathbf{w}$, which can be seen as a long-term memory.
There are many ways to construct $\hat{v}(s, \mathbf{w})$, and in this project we use \textit{deep neural networks}
as function approximators, which will be discussed in detail in section \ref{sec:deep_learning}.

In section \ref{sec:multi} we estimated the value function based on the n-step return, but
since the value function now depends on the weights, $\mathbf{w}$, we need to find a way
to measure how much each component of the weight vector is influencing the
current estimate.
To do so we can use an \textit{eligibility trace}, $\mathbf{e}^{\mathbf{w}} \in \R^d$, that
has exactly as many components as the weight vector.

Generally we want to use the elegibility trace to improve the value function by
changing its weights, $\mathbf{w}$, using \textit{gradient ascent}.
The eligibility trace describes the elegibility of its corresponding component in $\mathbf{w}$
- in other words it describes the recent tendency in directional changes needed to be made
to maximize $\hat{v}(s, \mathbf{w})$.
Thus an elegibility trace for the value estimator can be updated as
\begin{equation}
    \mathbf{e}^\mathbf{w} = \lambda \mathbf{e}^\mathbf{w} + \nabla_\mathbf{w} \hat{v}(s, \mathbf{w})
\end{equation}
where $\lambda$ is a discounting factor used to decrease the elegibility of a component
over time.
The trace describes the trend of the most recent gradients of the value function
with respect to $\mathbf{w}$, but we also want to base the update of the weights on their
performance.
Since we want to improve the estimate of the value function, as we experience the effect of our actions,
we update the weights proportionally to the one-step TD-error discussed in section \ref{sec:td}.
A low TD-error means that we don't have to change much, since the value estimator is
close to a maximum, while a high error means that we need to take a larger step
in the directions of the elegibility trace as we are still far from the maximum.
An update based on this approach is defined as
\begin{equation}
    \mathbf{w} = \mathbf{w} + \eta \delta \nabla e^\mathbf{w}
\end{equation}
where $\eta$ determines how much the weights can change each step.


\end{document}
