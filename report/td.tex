\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}

\maketitle

\subsection{Temporal Difference Learning}\label{sec:td}

We now have a way to describe the value of a state, but we seldomly know the
value function $v_\pi$ before we begin solving a problem.
Even with the knowledge of the entire state space and all available actions
it is difficult to compute the true value function.
Instead it is possible to construct an estimator $V(S_t)$ that
approximates $v_\pi(S_t)$.

When a learning agent solves a problem, it gains a lot of experience
about the states it encounters and rewards it earns through different transitions.
Using this experience it is possible to construct an updating scheme
for estimating the approximator,
\begin{equation}
    V(S_t) = V(S_t) + \alpha [G_t - V(S_t)]
\end{equation}
where $G_t$ is the experienced actual return following from state
$S_t$ and $0 \leq \alpha \leq 1$ is a step-size parameter, 
which defines how much each update is able to change the estimate
of the value in state $S_t$.

This estimator only allows us to update the estimate, when an epsiode
has ended, since this is the time at which $G_t$ is known.
Updating this way is inefficient because it would take the estimator
a lot of episodes to converge towards the true state-value function,
due to it being updated seldomly. 

Instead of updating the estimator $V(S_t)$ when an episode has
ended, we can choose a different approach which updates the $V(S_t)$
every time a new state is reached.
From equation \ref{eq:sv} we know that the value function is partly based on its future
estimate, which we can extend to form an update scheme,
\begin{equation}
    V(S_t) = V(S_t) + \alpha  [R_{t+1} + \gamma  V(S_{t+1}) - V(S_t)]
\end{equation}
where we only have to take a single step into the future before we are
able to update the estimate.
Updating the value estimator based on future results is the basis
of \textit{temporal difference learning}(TD) and the quantity $R_{t+1} + \gamma  V(S_{t+1}) - V(S_t)$
is defined as the \textit{one-step TD error}.
This error describes the difference between the current estimator
and the estimate that is based on actual current experience.

Generally we want to minimize the TD error, as the function $V(S_t)$
would then be as close to $v_\pi(S_t)$ as possible.
In particular if this error was 0 for all states, then
$V(S_t) = v_\pi(S_t)$ and
\begin{align}
   V(S_t)  & = V(S_t) + \alpha  [R_{t+1} + \gamma  V(S_{t+1}) - V(S_t)]   \\
           & = V(S_t) + \alpha  [V(S_t) - V(S_t)]  \\
           & = V(S_t) 
\end{align}

\subsubsection{Multi-step TD learning}

The benefit of this kind of updating scheme is that we are able to base
part of our estimate on an already existing estimate.
Sometimes it is useful to look at every state-change, but sometimes
changes over a single time step can be small and maybe even insignificant.
The one-step update scheme used a single experienced state transition to estimate
$G_t$ using the reward earned in the transition and the estimate of the
next state.
Basically we want to extend this way of estimating $G_t$ to take $n$ experienced rewards
into account, which is defined as the \textit{n-step} return,
\begin{equation}
    G^{(n)}_t = R_{t+1} + \gamma R_{t+2} + \dots 0 \gamma^n V(S_{t+n})
\end{equation}
where $n \geq 1$, $0 < t \leq T - n$ and $T - n$ is the nth time step before the terminal state
is reached.

Using the n-step return allows the learning agent to base its update on the next $n$ experienced
rewards, which means it is more likely to experience a significant change than the one-step return.
This way of updating the estimate is espicially beneficial in tasks that only sometimes returns a reward
- e.g. in a lot of games a score is only given to the player when an objective is completed.
We want to find these significant changes because the entire chain of estimates 
will be affected, making the estimate converge faster towards the real value function.

\subsubsection{Eligibility traces}\label{sec:eligibility_traces}

In section \ref{est_vals} we mentioned estimators being represented as
parameterized functions.
Using a parameterized function to estimate the state-value function means
the n-step return can be defined as
\begin{equation}
    G^{(n)} = R_{t+1} + \gamma  R_{t+2} + \gamma^2  R_{t+3} + \dots + \gamma^n  \hat{v}(S_{t+n}, \mathbf{\theta})
\end{equation}
where $\hat{v}(S_{t+n}, \mathbf{\theta}_t)$ is an estimator of the
state-value function with parameters $\mathbf{\theta}_t$.
Instead of keeping track of the last $n$ steps as in the n-step TD
methods,
we can construct a short-term memory vector called an \textit{eligibility trace}, $\mathbf{e}_t \in \R^n$,
that parallels the weight vector $\mathbf{\theta}_t \in \R^n$\cite{RLbook}.
This eligibility trace describes the eligibility of each component in $\mathbf{\theta}_t$ -
how much influence this particular component has in the estimation.
Another advantage of using an eligibility trace is that
learning occurs continually and can affect behaviour immediately,
unlike n-step methods which are always $n$ steps behind.

The eligibility of a component of the weight vector is determined
by its contribution to the most recent computations of the estimate,
where “recent” is determined by the discount rate $0 < \gamma \leq 1$ and the weight
$0 < \lambda \leq 1$.
Thus whenever a component of the weight vector
is used to produce an estimate, the corresponding eligibility component
is increased in either a positive or negative direction and then begins to fade away.
%If a component is rarely used, we provide it with less eligibility
%as it is then less likely to be directly affecting the estimate, and vice versa if
%a component is used often.
This directional change can be found by computing the
gradients of the estimator with respects to $\mathbf{\theta}_t$\cite{RLbook}.
An eligibility trace for $\hat{v}(S_{t+n}, \mathbf{\theta}_t)$ would then
be maintained in the following fashion

\begin{equation}
    \mathbf{e}_t = \nabla(\hat{v}(S_t, \mathbf{\theta}_t)) + \lambda  \gamma  \mathbf{e}_t
\end{equation}
with an initial value of 0 for every component.
The weights of the estimator $\hat{v}(S_t, \mathbf{\theta}_t)$ are
updated every time a state transition occurs, which produces
produces the one-step TD error, $\delta_t = R_{t+1} + \gamma  \hat{v}(S_{t+1}, \mathbf{\theta}_t) - \hat{v}(S_t, \mathbf{\theta}_t)$.
The weights of the estimators we use are updated proportionally to the TD error, since
a low error means we don't have to change much while a large error means that
the estimator needs to be corrected significantly.
A typical update scheme could look like this
\begin{equation}
    \mathbf{\theta}_t = \mathbf{\theta}_t + \eta  \delta_t  \nabla(\hat{v}(S_t, \mathbf{\theta}_t))
\end{equation}
where $0 < \eta \leq 1$ is a step-size parameter limiting how much the weights,
$\mathbf{\theta}_t$, can be changed each time they are updated.


\end{document}
