\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}

\maketitle


\subsection{Estimating the value of states}\label{est_vals}

We want to be able to tell whether it is good or bad to
perform an action in a state.
Therefore we need a way to assign a \textit{value} to a state,
since we can then choose which action to perform
based on the \textit{expected} total reward the action provides.

Now, to assign a value to a state we need to take the rewards
gained from all future state transition into account.
From a state $S_t$ this quantity is the \textit{expected} sum of
future rewards from $S_t$,
\begin{equation}\ref{G_t}
    \mathds{E}[R_{t+1} + R_{t+2} + \hdots + R_{T} | S_t] = \mathds{E}[\sum\limits_{i = 1}^T R_{t+i} | S_t]
\end{equation}
where $T$ is the point in time where the \textit{terminal state} is reached.
The terminal state is the last state available in the problem environment and
no other states can be reached from this state.
That is, in backgammon the terminal state is the state where one of the players
have removed all of their pieces from the board and the game is over.
In this project we will only be solving \textit{episodic} problems, which
means there will always be a terminal state.
However, using the sum of rewards as the return of a state proves problematic.
In a game of backgammon, for example, where a player is given reward one for performing a winning move
and reward zero for all other actions, he return of all states will be
the value of $R_T$ because the action performed at this time is the only one
that can trigger a reward.
Therefore the learning agent won't be able to tell which actions are best,
since all state transitions, except the last, will result in the same reward.
To deal with this issue we use the \textit{discounted} sum of rewards as our return instead
of just the sum.
This means that for each time step further into the future
we proportionally discredit the reward, which means the value
of a state $s$, $v_\pi(s)$, can now be described as
\begin{equation}\label{gammaG_t}
    \begin{aligned}
        v_\pi(s) &= \mathds{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} \cdots | S_t = s]\\
        &= \mathds{E}[ \sum\limits_{k = 0} \gamma^k R_{t+k+1} | S_t = s]\\
        &= \mathds{E}[ G_t | S_t = s]
    \end{aligned}
\end{equation}
where $ 0 \leq \gamma \leq 1$ is the rate of discount that determines how far-sighted the learning agent can be,
with a rate of zero meaning the return is the reward obtained in the first transition from $s$ and a rate of
one giving us equation \ref{G_t}.

According to the agent-environment model (Fig. \ref{fig:agent_environment}) the response that
the learning agent receives for performing an action consists
of the new state $S_{t+1} = s'$ and reward $R_{t+1} = r$ earned in the transition
to $s'$.
Assuming the response depends on all past actions $A_{0}, A_{1}, \cdots, A_{t-1}$ and
corresponding responses $S_{1}, R_{1}, \cdots, S_{t}, R_{t}$, the probability
of arriving in state $S_{t+1}$ and receiving reward $R_{t+1}$ can be defined as the
joint probability given by
\begin{equation}\label{eq:joint}
    \mathds{P}(S_{t+1} = s', R_{t+1} = r | S_0, A_0, R_1, \cdots, S_{t-1}, A_{t-1}, R_{t}, S_{t}, A_{t})
\end{equation}
where $S_0$ is the inital state and $A_0$ is the first action taken.

This notation is tedious, so to avoid it, we will assume that all Reinforcement Learning tasks
we encounter possess the \textit{Markov property}.
The Markov Property is present in a task if the current state contains
enough information to make remembering previous states unimportant.
This property allows us to describe the joint probability from equation \ref{eq:joint}
as
\begin{equation}\label{joint_prob}
    \mathds{P}(S_{t+1} = s', R = r | S_t = s, A_t = a)
\end{equation}
since having the information from $S_t$ makes retaining information about previous states
and state transitions unnecessary.

The expectation of the return depends on the probabilities of taking action $a$,
encountering the new state $s'$ and receiving reward $r$.
Therefore the \textit{state-value function}, $v_\pi(s)$ can be defined recursively as
\begin{equation}
    \begin{aligned}
        v_\pi(s) &= \mathds{E}[ G_t | S_t = s]\\
        & = \mathds{E}_\pi[R_{t+1} + \gamma \sum\limits_{k=0} \gamma^k R_{t+k+2} | S_t = s]\\
        & = \sum\limits_{a} \pi(a|s) \sum\limits_{s', r} p(s', r | s, a) [r + \gamma \mathds{E}_\pi[\sum\limits_{k=0} \gamma^k R_{t+k+2} | S_{t+1} = s']\\
        & =  \sum\limits_{a} \pi(a|s) \sum\limits_{s', r} p(s', r | s, a) [r + \gamma \mathds{E}_\pi[G_{t+1} | S_{t+1} = s']\\
        & =   \sum\limits_{a} \pi(a|s) \sum\limits_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]
    \end{aligned}
\end{equation}
where $p(s', r | s, a)$ is the shorthand notation for the joint probability from equation \ref{joint_prob} and $\pi(a|s)$ is the
probability of taking action $a$ in state $s$ following policy $\pi$.

%\printbibliography
%\bibliography{citations}
%\bibliographystyle{plain}
\end{document}
