\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}

\maketitle

\section{Overview of RL} 

\begin{enumerate}
    \item What is RL?
    \item What is the setting? (State, Action)
    \item What is a policy?
    \item What is a reward?
    \item How to you estimate the value of being in a state?
    \item How do you estimate the value of taking an action? (action-value)
    \item Looking into the future (TD).
    \item TD-Error and what it can be used for.
    \item Do we even need to estimate action-value/value functions?
    \item Policy Gradient Methods.
    \item Policy Approximation
    \item Actor-critic in general.
    \item The one-step actor-critic method.
    \item (MAYBE) Actor-Critic using eligibility traces
    \item Asynchronous Actor-Critic
    \item Other (asynchronous) methods (SARSA, DQN, Q-learning).
\end{enumerate}

\end{document}
