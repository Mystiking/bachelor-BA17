\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}

\maketitle


\section{Reinforcement Learning}



\subsection{Temporal Difference (TD) Learning}

Temporal difference (TD) learning is a technique used in reinforcement learning for predicting the total reward expected in the future. By using TD learning it's possible to update the value functions after each time step. We wish to update the value functions after each time step, instead of each episode, for stabilizing the learning with several minor updates. Updating the state value function for a state $S_{t}$ at time $t$ after running through an entire episode can be formalized as,
\begin{equation}\label{eq:td}
    V(S_{t}) \leftarrow V(S_{t}) + \alpha \big[G_{t} - V(S_{t}) \big]
\end{equation}
where $0 < \alpha \leq 1$ corresponding to the update rate of the value state function. The problem with this approach is that we have to wait until an episode is done, before we can update the state value function. We want to update the state value function dynamically, because we assume that the next estimation of the value is closer to the true return. One way for updating the value function dynamically is to estimate the value of a state and compare it to the estimated value in the subsequent state, this method is called \textit{TD(0)}.
\begin{equation}\label{eq:td2}
    V(S_{t}) \leftarrow V(S_{t}) + \alpha \big[R_{t + 1} + \gamma V(S_{t + 1}) - V(S_{t}) \big]
\end{equation}
Here we compute the difference between the estimated value for being in state $S_{t}$ and $S_{t + 1}$ plus $R_{t + 1}$, where $R_{t + 1}$ is the reward gained by the transition from state $S_{t}$ to $S_{t + 1}$. The expression $R_{t + 1} + \gamma V(S_{t + 1}) - V(S_{t})$, is called the \textit{TD error} for TD(0).
\begin{equation}
    \delta_{t} = R_{t + 1} + \gamma V(S_{t + 1}) - V(S_{t})
\end{equation}
So the generally in TD error is that $(V(S_{t + 1}) + R_{t + 1})$ should be a better estimate of the actually return than $V(S_{t})$ for state $S_{t}$, by the assumption of that the estimate made later in the episode is better, because it will be closer to a terminal state.  

\subsection{$n$-step return}
We mentioned TD(0) in the last section, is called one-step, because it change the value functions based on a estimate made one step later. TD learning can also be used for \textit{n-step backups}, where we compute the TD error between state $S_{t}$ and $S_{t + n}$. The $n$-step backup can formally be expressed as the $n$-step return,
\begin{equation}\label{eq:return}
    G_{t}^{(n)} = R_{t + 1} + \gamma R_{t + 2} + \dots + \gamma^{n - 1} R_{t + n} + \gamma^{n} V_{t + n - 1}(S_{t + n})
\end{equation}
where $n \geq 1$ is the number of time steps we look intro the future. 

\subsection{Eligibility Traces}
As mentioned earlier we are going to use a neural network for approximate the value functions and policy. So we rewrite equation \ref{eq:return}, so it use the parameters from the neural network,
\begin{equation}
    G_{t}^{(n)} = R_{t + 1} + \gamma R_{t + 2} + \dots + \gamma^{n - 1} R_{t + n} + \gamma^{n} \hat{v}(S_{t + n}, \theta_{t + n - 1})
\end{equation}
where $\theta$ is the parameter used for approximate the value, in a neural network it correspond to the weights in the network. 

When performing function approximation on the value function and the policy, we wish that the parameters $\theta$ are updated often for stabilizing learning, and that our algorithm works on continuing problems. For contain the information needed for making dynamic updates in a memory efficient way, we using \textit{eligibility traces}. An eligibility trace is a vector $\mathbf{e} \in \mathds{R}^{n}$ with same shape as the weight vector $\theta_{t}$. The eligibility trace can bee seen as a short-terms memory which contain information about how the weights are updates in the last episode or $n$ time step, versus the weight vector which contain our current weights using all the updates performed over time. Often a eligibility trace vector is initialized to zero at the beginning of an episode.
\begin{equation}
  \mathbf{e}_{0} = 0  
\end{equation}
At each time step it is incremented by the gradient of the value function and multiplied with some discounting parameter $\lamda$ so earlier updates fades away.
\begin{equation}
  \mathbf{e}_{t} = \nabla_{\hat{v}}(S_{t}, \theta_{t}) + \lamda \mathbf{e}_{t - 1}
\end{equation}






\end{document}
