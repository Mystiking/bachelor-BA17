\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}

\maketitle

\subsection{Using deep learning for approximating functions}

In the last section we claimed that a neural network could be used to approximate the
state-value and action-value functions.
When we refer to a neural network, what we mean is an artificial neural network -
a computational model that emulates connections between neurons in the human brain.
Formally this means that we wish to find the functions $v_\pi^{\sim}(s)$ and $q_\pi^{\sim}(s,a)$ that
satisfies $v_\pi^{\sim}(s) \approx v_\pi(s)$ and $q_\pi^{\sim}(s,a) \approx q_\pi(s, a)$.
In general this means that for a function $f(x)$, the neural network tries to approximate
$f^{\sim}(x)$ such that $f^{\sim}(x)$ varies the least from $f(x)$ for all $x$ in the input
space\cite{DeepLearningBook}.
It does so by adjusting its parameters based on experience - data for which $f(x)$ is already known -
that it can use to compute how well the current approximation is performing.

This data is exactly what is available to us in the agent-environtment model, when there is a natural terminal state,
since we can then use the experienced actual returns to update the weights of the models.

\subsubsection{Layers, neurons and connections}

The goal of deep learning is to separate difficult problems into smaller and simpler problems
and in this process create a hierarchy of concepts, where each concept can be described in terms
of simpler concepts\cite{DeepLearningBook}.

More formally, a neural network consists of \textit{layers} that each represent a concept, which
when connected are able to solve the complex problem at hand.
With this definition of a neural network the approximation function $f^{\sim}(x)$ can be
defined as
\begin{equation}
    f^{\sim}(x) = f^{k}(f^{k-1}( \cdots f^{0}(x) \cdots))
\end{equation}
for a network with $k$ layers.
The only layers that have their dimensions restrained are the first and last, since they are defined by the
dimensions of the input and the output of the approximator function.
This means the layers inbetween can have arbitrary shapes and
since we won't be seeing the output of those layers, we call them \textit{hidden layers}.

Each layer consists of a number of \textit{neurons} that are connected to the neurons of the
next layer.
%%%% Figure of layers
\begin{figure}[!h]
    \centering
    \includegraphics[width=10cm]{include/layers.png}
    \caption{A series of layers in a fully connected neural network.}
    \label{fig:layers}
\end{figure}
When we discuss layers in this project we will only be referring to \textit{dense}
layers where every neuron is connected to all neurons in the subsequent layer - a fully
connected neural network.

Each neuron transforms its series of input signals into a single output signal which is sent to
each neuron in the following layer through a \textit{weighted connection}, where
the weight of the connections define the importance of the signals.
Thus purpose of the hidden layers is to extract the key features of the original input.
%%%% A drawing of a neuron “up close”
\begin{figure}[!h]
    \centering
    \includegraphics[width=10cm]{include/neuron.png}
    \caption{The structure of a single neuron.}
    \label{fig:neuron}
\end{figure}

%% Introduce activators

\subsubsection{Updating the weights using gradients}

\end{document}
