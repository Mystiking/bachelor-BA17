\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}

\maketitle

\section{Data}\label{Data}

To train and test our Actor-Critic and asynchronous advantage Actor-Critic implementation we will use
the OpenAI Gym framework\cite{openAIGym}.
This framework provides an interface to 118 Atari 2600 games,
that can be used by developers to compare and test their reinforcement learning algorithms.

 
In the Atari enviroments there is always a finite \textit{action space} and continous \textit{state space}.
The state space is continous because a \textit{state} is represented as a RGB image - a screenshot from a game
at the given time. The action space is finite because there is always a fixed amount of
actions available to the player in the OpenAI Gym framework.

Simulating an action in a game takes the player into a new state
and returns information about the new state,
the \textit{reward} the action trigered and a \textit{binary signal} which indicates whether the 
game is done or not.
The reward is the score gained from transitioning to the new state and
it is not uncommon for this reward to be $0$ as many of the players actions do not produce immediate rewards.
In most of the Atari games performing an action for a single frame have almost no impact,
so instead taking an action using the OpenAI Gym framework
results in that action being repeated $x$ times, where $x$ is uniformly sampled from $\{2, 3, 4\}$.

\subsection{Cart-Pole}

Among a lot of other game types OpenAI gym also supports an interface
to the Cart-Pole problem.
A player can win the Cart-Pole game if they manage to balance a pole connected to the top of a moving cart for
200 consecutive time steps.
The player loses if the pole moves more than 15 degrees from an upright position in
either direction or if the cart exits the frame.

\begin{figure}[!h]
    \centering
    \fbox{\includegraphics[scale=0.5]{include/cartpole.png}}
    \caption{A frame from Cart-Pole.}
    \label{fig:cartpole}
\end{figure}

Unlike the Atari environments a state in Cart-Pole consits of only four elements - the position and velocity of the cart, and the
angle and angular velocity of the pole.
Each time step, corresponding to a new frame, the player has to chose between moving the cart to the right or to the left and it is not possible
to do nothing.
For every time step the pole hasn't moved more than 15 degrees from the starting position and
the cart has stayed inside of the frame, the player is awarded a single point.

We will be using the Cart-Pole problem as a proof of concept for our Actor-Critic implementation and will
compare it to the A3C algorithm used on the same problem.
The reason for using the Cart-Pole problem is that it should be easier
to solve, because each state consists of fewer dimensions than a state
from an Atari game.


\subsection{Atari 2600 games}\label{sec:atari}

A state in an Atari game is represented as a 3-dimensional matrix representing a
frame.
This means that instead of only taking 4 elements into consideration when picking an action,
the player is now faced with a much more complex view. 
In general we want the learning agent to be able to pick an action from all
the available actions, but in some of the Atari games there are duplicate actions.
To speed up the training we have chosen to remove duplicates of the same action
from the action space.

All of the games have the same resolution of $210 \times 160$ pixels.
Each pixel contains three channels corresponding to the red, green and blue intensities of the pixel.
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.35]{include/space_invaders_1.png}
    \caption{A raw RGB frame from \textit{Space Invaders}.}
    \label{fig:si}
\end{figure}

Using the raw Atari frames can be computationally demanding, so we have chosen
to preprocess our data as described in \cite{dqn}. 
The first step of the preprocessing is to reduce the amount of channels used to
represent the image.
To do this we gray-scale the RGB representation of the frames that only a single channel is used.
In our approach, the gray-scaling is calculated based on the relative luminance\cite{luminance} of each pixel,
for which the formula is given in equation \ref{eq:lum}.
\begin{equation}\label{eq:lum}
    L(R, G, B) = 0.2126*R + 0.7152*G + 0.0722*B
\end{equation}

We further reduce the dimensions of the gray-scaled images by a downsampling, where
we simply ignore every second pixel, effectively halving the resolution of the frames 
in both dimensions.
To avoid the scoreboard influencing the training, we crop the downsampled image by removing
15 rows of pixels from the top of the screen.
This step is only possible because we have selected games, where only the scoreboard is
removed by this method.
\begin{figure}[h!]
    \begin{subfigure}[t]{.5\textwidth}
        \centering
        \includegraphics[scale=0.35]{include/space_invaders_1_gray.png}
        \caption{The gray-scaled frame from figure \ref{fig:si}.}
        \label{fig:sig}
    \end{subfigure}
    \begin{subfigure}[t]{.5\textwidth}
        \centering
        \includegraphics[scale=0.35]{include/space_invaders_1_gray_resized.png}
        \caption{The result of resizing the gray-scaled frame from figure \ref{fig:sig}.}
        \label{fig:scanlike}
    \end{subfigure}
    \caption{The two steps of the preprocessing applied to the raw frame from figure \ref{fig:si}.}
\end{figure}


The final output of the preprocessing step is a $90 \times 80$ pixel gray-scaled
image.
By preprocessing the data, the remaining featuer extraction becomes computationally much
cheaper, as the dimensions have been significantly reduced compared to the raw Atari frames.

In the articles \cite{a3c} and \cite{dqn} one more preprocessingstep is performed.
Instead of only resizing the gray-scaled frames, they are also cropped to $84 \times 84$ images.
This is done to meet a condition in their implementation of 2d convolutions,
since they could only process square input\cite{dqn-nature}.
Our cropping scheme difers from their, due to the purpose being another.

Due to the time limitations of this project we will only be testing our
implementation of the A3C algorithm on certain games;
namely \textit{Pong}, \textit{Breakout}, \textit{Space Invaders} and
\textit{Pacman}.

\end{document}
