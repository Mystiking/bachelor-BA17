\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}

\maketitle

\section{Reinforcement Learning}

Reinforcement Learning is a machine learning technique that attempts 
to map situations to actions.
%%%% Er det her en god formulering?
It does so by learning from interaction, a process which can be described as trying to solve a problem,
without knowing the “rules” of the problem.
Using the experience gained from trying different moves it is able to learn how to solve it.
In other words when a good action is found for a given situation,
the probability of taking said action needs to be reinforced because it is then more likely to be
repeated in the future for a similar situation.

The first task of learning to solve a problem with reinforcement learning is to define
the \textit{states} and \textit{actions} of the problem environment.
A state contains all available information related to the environment,
i.e. in a game of chess a state could contain the position of every piece on the board at
a given time.

An action describes a transition from one state to another.
In the chess example an action would be to move a piece, following the rules of the
environment.

When a reinforcement learning algorithm is learning to solve a task, it is
interacting with the environment through a \textit{learning agent}.
The agent decides which action should be performed in each state by creating and maintaining a \textit{policy}.
The policy is a mapping from states in the environment to actions available to the agent\cite{RLBook}.
Policies can take many forms, such as simple look-up table where each state map to an
action, but in this project we will be treating policies solely as probability distributions.
This means that a policy $\pi(A_t = a|S_t = s)$ given an action $a$ and a state $s$ describes the
probability of taking action $a$ in state $s$ under policy $\pi$ at time $t$.

This means that every problem can be described as a set of states that can be reached
through a set of actions describing how to interact with the environment.
The difficult part of reinforcement learning is choosing the best action in a
state.
To do so the agent is rewarded for making good decisions.
This means that transitioning from one state to another should be assigned a numerical value
- a \textit{reward} - that describes if the action was good or bad.

%%%% Actor-Environment graph
\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.5]{include/RLdiagram.png}
    \caption{A representation of the workflow in an agent-environment
    model. Here the environment sends a state and reward signal to the
    learning agent which then responds with a new action and so forth.}
    \label{fig:agent_enviroment}
\end{figure}
%%%%

The essence of a good reinforcement learning algorithm is that it should be able to learn
how to solve a task satisfactory without any prior knowledge of the rules of the environemnt.
To do so the agent uses a combination of \textit{exploration} and \textit{exploitation}.
The goal of the learning agent is to learn a policy that maximises the total reward, but in order to do
so it must explore some of the options that seem suboptimal at first.
This is because the immediate reward might be lower but it may lead to a higher total reward in the end.
The learning agent shouldn't be exploring too much though, since it then wouldn't
be using the previously gathered knowledge - in other words it needs to
exploit the fact that it “knows” the result of taking an action since it has been in
a similar situation before.
There needs to be a balance between exploration and exploitation for the
agent to learn a policy that maximizes the final reward.

\end{document}
