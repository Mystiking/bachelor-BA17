\documentclass[11pt]{article}
\usepackage{mypackages}
\begin{document}

\section{Conclusion}

The aim of this project was to investigate the effects of asynchronous
training in Deep Reinforcement Learning.
A basic Actor-Critic method was presented,
as well as the more recent Asynchronous Advantage Actor-Critic algorithm.
We were able to solve the CartPole problem and several games from the
Atari 2600 system.
The results show that it is possible to obtain a
speed-up of 631,27\% on average by using 16 learning agents in parallel, compared to
using only a single thread, in the Atari games.
However, we only saw a speed-up of 15,09\% in the time it took for the CartPole problem
to complete 200.000 timesteps for the best amount of threads.
This indicates that the advantages of asynchronous training are only present when
there is a balance between the amount of time used interacting with the environment
and the time spent performing the asynchronous update.
Thus, in further work, it would be interesting to examine how increasing
both the number of threads and the amount of work the threads can perform between each
asynchronous update, can influence the pace of learning for the A3C method.

\end{document}
